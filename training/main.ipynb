{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Note: The following do not work with Python 3.12\n",
    "#import sweetviz as sv\n",
    "#import shap\n",
    "#from ydata_profiling import ProfileReport\n",
    "\n",
    "\n",
    "seed = 2024  #seed = 2024: train model as stated in example_crisp_dm_pipeline.ipynb\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Data Cleaning: Readin data and preprocessing individual table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes = pd.read_csv('data/recipes.csv')\n",
    "\n",
    "# Consolidated Non-Vegetarian Keywords\n",
    "non_vegetarian_keywords = list(set([\n",
    "    'flounder', 'lobsters', 'lump', 'rack', 'shank', 'steak', 'scallops', 'alligator', \n",
    "    'livers', 'roe', 'ham', 'turkey', 'chicken', 'duck', 'bacon', 'tuna', 'swordfish', \n",
    "    'lobster', 'meatballs', 'salmon', 'sweetbreads', 'breasts', 'chicken-flavored', \n",
    "    'ducklings', 'drumstick', 'liver', 'shanks', 'rabbit', 'poultry', 'herring', \n",
    "    'mussels', 'clams', 'squid', 'pork', 'veal', 'haddock', 'chorizo', 'chihuahua', \n",
    "    'eel', 'stuffing', 'cod', 'gelatin', 'sausage', 'curd', 'thighs', 'lox', 'cabbage', \n",
    "    'wonton', 'bone', 'giblets', 'pheasant', 'quail', 'shrimp', 'fish', 'sole', \n",
    "    'gizzard', 'Canadian', 'pesto', 'truffles', 'anchovies', 'venison', 'pheasants', \n",
    "    'tenderloin', 'meats', 'tripe', 'breast', 'wings', 'ribs', 'sausages', 'trout', \n",
    "    'oysters', 'octopus', 'crab', 'prawns', 'catfish', 'sardines', 'mahi', 'halibut', \n",
    "    'bass', 'perch', 'tilapia', 'grouper'\n",
    "]))\n",
    "\n",
    "# Consolidated Non-Vegan Keywords\n",
    "non_vegan_keywords = list(set([\n",
    "    'milk', 'cheese', 'butter', 'egg', 'honey', 'mozzarella-cheddar', 'cream', 'whip', \n",
    "    'jarlsberg', 'fontina', 'ham', 'cheesecake', 'hollandaise', 'caviar', 'creamRegular', \n",
    "    'custard', 'yogurt', 'gouda', 'margarine', 'beef', 'salmon', 'sour', 'bisquick', \n",
    "    'carton', 'cotija', 'creme', 'buttercream', 'buttermilk', 'ricotta', 'cottage', \n",
    "    'eggs', 'mayonnaise', 'eggshells', 'lactose-free', 'skim', 'ghee', 'mascarpone', \n",
    "    'alfredo', 'whey', 'casein', 'lactose', 'albumin', 'bechamel', 'sour cream', \n",
    "    'cream cheese', 'feta', 'gorgonzola', 'parmesan', 'mozzarella', 'cheddar', 'brie', \n",
    "    'camembert', 'roquefort', 'stilton', 'blue cheese', 'colby', 'monterey jack', \n",
    "    'swiss cheese', 'provolone', 'edam', 'havarti', 'pecorino', 'asiago', 'emmental', \n",
    "    'gruyere', 'halloumi', 'manchego', 'paneer', 'queso fresco', 'ricotta salata', \n",
    "    'romano', 'taleggio', 'vacherin', 'milk chocolate', 'whey protein', 'casein protein', \n",
    "    'egg noodles', 'egg whites', 'egg yolks', 'hollandaise sauce', 'aioli', 'flan', \n",
    "    'quiche', 'meringue', 'pavlova', 'egg wash', 'frittata', 'omelette', 'scrambled eggs', \n",
    "    'poached eggs', 'hard-boiled eggs', 'deviled eggs', 'eggnog', 'brioche', 'challah', \n",
    "    'pound cake', 'sponge cake', 'angel food cake', 'ladyfingers', 'mousse', 'souffle', \n",
    "    'creme brulee', 'panna cotta', 'tiramisu', 'yorkshire pudding', 'beef broth', \n",
    "    'chicken broth', 'fish sauce', 'oyster sauce', 'worcestershire sauce', 'caesar dressing', \n",
    "    'carbonara sauce', 'bÃ©arnaise sauce', 'gravlax', 'smoked salmon', 'caviar', 'anchovy paste', \n",
    "    'fish stock'\n",
    "]))\n",
    "\n",
    "\n",
    "# Function to check if a RecipeIngredientParts is vegetarian\n",
    "def is_vegetarian(ingredient):\n",
    "    for keyword in non_vegetarian_keywords:\n",
    "        if keyword in ingredient.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Function to check if a RecipeIngredientParts is vegan\n",
    "def is_vegan(ingredient):\n",
    "    for keyword in non_vegan_keywords:\n",
    "        if keyword in ingredient.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Apply the is_vegetarian function to the RecipeIngredientParts column\n",
    "df_recipes['is_vegetarian'] = df_recipes['RecipeIngredientParts'].apply(is_vegetarian)\n",
    "\n",
    "# Apply the is_vegan function to the RecipeIngredientParts column\n",
    "df_recipes['is_vegan'] = df_recipes['RecipeIngredientParts'].apply(is_vegan)\n",
    "\n",
    "# Map the diet category based on the is_vegetarian and is_vegan columns\n",
    "df_recipes['diet_category'] = df_recipes.apply(lambda row: 'Vegetarian' if row['is_vegetarian'] else 'Vegan' if row['is_vegan'] else 'Omnivore', axis=1)\n",
    "\n",
    "# create TotalTime_Recipe column\n",
    "df_recipes['TotalTime_Recipe'] = df_recipes['CookTime'] + df_recipes['PrepTime']\n",
    "\n",
    "# drop columns\n",
    "df_recipes = df_recipes.drop(columns=['Name', 'CookTime', 'PrepTime', 'RecipeIngredientParts', 'RecipeIngredientQuantities', 'RecipeYield', 'is_vegetarian', 'is_vegan'])\n",
    "# dtype conversion\n",
    "df_recipes[\"RecipeCategory\"] = df_recipes[\"RecipeCategory\"].astype(\"category\")\n",
    "df_recipes[\"diet_category\"] = df_recipes[\"diet_category\"].astype(\"category\")\n",
    "# rename columns\n",
    "df_recipes = df_recipes.rename(columns={\"diet_category\": \"recipe_diet_category\"})\n",
    "\n",
    "#df_recipes.info()\n",
    "#df_recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AuthorId    0\n",
      "Diet        1\n",
      "Age         0\n",
      "dtype: int64\n",
      "['Vegetarian' 'Vegan' 'Omnivore' nan]\n",
      "AuthorId    0\n",
      "Diet        0\n",
      "Age         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_diet = pd.read_csv('data/diet.csv')\n",
    "\n",
    "# chcek for missing values in the data\n",
    "print(df_diet.isnull().sum())\n",
    "\n",
    "# replace missing value in Diet with \"Omnivore\"\n",
    "print(df_diet[\"Diet\"].unique())\n",
    "df_diet[\"Diet\"] = df_diet[\"Diet\"].fillna(\"Omnivore\")\n",
    "\n",
    "# check again\n",
    "print(df_diet.isnull().sum())\n",
    "\n",
    "# Change data type of Diet to category\n",
    "df_diet[\"Diet\"] = df_diet[\"Diet\"].astype(\"category\")\n",
    "\n",
    "# rename the column Diet to diet_category\n",
    "df_diet = df_diet.rename(columns={\"Diet\": \"user_diet_category\"})\n",
    "\n",
    "#df_diet.info()\n",
    "#df_diet.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AuthorId        0\n",
      "RecipeId        0\n",
      "Time            0\n",
      "HighCalories    0\n",
      "HighProtein     0\n",
      "LowFat          0\n",
      "LowSugar        0\n",
      "HighFiber       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_requests = pd.read_csv('data/requests.csv')\n",
    "\n",
    "# check for missing values\n",
    "print(df_requests.isnull().sum())\n",
    "\n",
    "#dtype\n",
    "df_requests['HighCalories'] = df_requests['HighCalories'].astype('boolean')\n",
    "\n",
    "df_requests['HighProtein'] = df_requests['HighProtein'].replace({'Indifferent': False, 'Yes': True})\n",
    "df_requests['HighProtein'] = df_requests['HighProtein'].astype('boolean')\n",
    "\n",
    "df_requests['LowFat'] = df_requests['LowFat'].astype('boolean')\n",
    "\n",
    "df_requests['LowSugar'] = df_requests['LowSugar'].replace({'Indifferent': False, '0': True})\n",
    "df_requests['LowSugar'] = df_requests['LowSugar'].astype('boolean')\n",
    "\n",
    "df_requests['HighFiber'] = df_requests['HighFiber'].astype('boolean')\n",
    "\n",
    "# rename columns\n",
    "df_requests.rename(columns={'Time': 'TotalTime_Requested'}, inplace=True)\n",
    "df_requests.rename(columns={'HighCalories': 'HighCalories_Requested'}, inplace=True)\n",
    "df_requests.rename(columns={'HighProtein': 'HighProtein_Requested'}, inplace=True)\n",
    "df_requests.rename(columns={'LowFat': 'LowFat_Requested'}, inplace=True)\n",
    "df_requests.rename(columns={'LowSugar': 'LowSugar_Requested'}, inplace=True)\n",
    "df_requests.rename(columns={'HighFiber': 'HighFiber_Requested'}, inplace=True)\n",
    "\n",
    "#df_requests.info() \n",
    "#df_requests.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kd/4w5fjp_536z4vbs4r30zh2ym0000gn/T/ipykernel_25188/2994343210.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_reviews = pd.read_csv('data/reviews.csv')\n"
     ]
    }
   ],
   "source": [
    "df_reviews = pd.read_csv('data/reviews.csv')\n",
    "\n",
    "#sns.countplot(data=df_reviews, x='Rating')  # Rating is only 2 except 2 rows -> drop Rating column\n",
    "df_reviews = df_reviews.drop('Rating', axis=1)\n",
    "\n",
    "# check for missing values\n",
    "# print(df_reviews.isnull().sum())\n",
    "\n",
    "# dtype \n",
    "df_reviews['Like'] = df_reviews['Like'].astype('boolean')\n",
    "\n",
    "#df_reviews.info()\n",
    "#df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aggregation (Merge the tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep all request, add info about custormers diet when exit -> df_diet right_join df_requests\n",
    "merged_df_diet_request = df_diet.merge(df_requests, on='AuthorId', how='right')\n",
    "#merged_df_diet_request.head(100)\n",
    "\n",
    "# request without matched recipe, or recipe without request is useless  -> normal join \n",
    "merged_df_diet_request_recipes = merged_df_diet_request.merge(df_recipes, on='RecipeId')\n",
    "#merged_df_diet_request_recipes.tail(100)\n",
    "\n",
    "# review without request,recipes is useless -> left \n",
    "merged_df_diet_request_recipes_reviews = merged_df_diet_request_recipes.merge(df_reviews, on=['RecipeId', 'AuthorId'], how='left')\n",
    "#merged_df_diet_request_recipes_reviews.info()\n",
    "\n",
    "merged_df = merged_df_diet_request_recipes_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Data Cleaning (after merged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ishanjainoffical.medium.com/choosing-the-right-correlation-pearson-vs-spearman-vs-kendalls-tau-02dc7d7dd01d\n",
    "def plot_corr(df, title, is_like=True):\n",
    "    if 'Like' in df:\n",
    "        df = df[df['Like'] == 1]\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(25, 7))\n",
    "    nutrients_corr = df.corr(method='kendall') \n",
    "    mask = np.triu(np.ones_like(nutrients_corr, dtype=bool))\n",
    "    cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "    sns.heatmap(nutrients_corr, mask=mask, cmap=cmap, annot=True, fmt=\".2f\", ax=ax1, center=0)\n",
    "    ax1.set_title(title + ' - kendall', fontsize=16)\n",
    "    nutrients_corr = df.corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(nutrients_corr, dtype=bool))\n",
    "    cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "    sns.heatmap(nutrients_corr, mask=mask, cmap=cmap, annot=True, fmt=\".2f\", ax=ax2, center=0)\n",
    "    ax2.set_title(title + ' - pearson', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Calories': 1304, 'FatContent': 76, 'SaturatedFatContent': 30, 'CholesterolContent': 290, 'SodiumContent': 2219, 'CarbohydrateContent': 138, 'FiberContent': 13, 'SugarContent': 72, 'ProteinContent': 54}\n"
     ]
    }
   ],
   "source": [
    "# Filter 5% outliers for each recipes nutrient\n",
    "df_nutrients = merged_df[['Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent',\n",
    "                          'HighCalories_Requested', 'HighProtein_Requested', 'LowFat_Requested', 'LowSugar_Requested', 'HighFiber_Requested', 'Like']]\n",
    "\n",
    "threshold_map = {}\n",
    "for col in df_nutrients.columns:\n",
    "    if col in ['HighCalories_Requested', 'HighProtein_Requested', 'LowFat_Requested', 'LowSugar_Requested', 'HighFiber_Requested', 'Like']:\n",
    "        continue\n",
    "    for i in range(10000):\n",
    "        threshold = i\n",
    "        percent = (df_nutrients[df_nutrients[col] > threshold][col].count() / df_nutrients[col].count())*100\n",
    "        if percent <= 5:\n",
    "            threshold_map[col] = threshold\n",
    "            break\n",
    "print(threshold_map)\n",
    "\n",
    "merged_df = merged_df[merged_df['Calories'] < threshold_map['Calories']]\n",
    "merged_df = merged_df[merged_df['FatContent'] < threshold_map['FatContent']]\n",
    "merged_df = merged_df[merged_df['SaturatedFatContent'] < threshold_map['SaturatedFatContent']]\n",
    "merged_df = merged_df[merged_df['CholesterolContent'] < threshold_map['CholesterolContent']]\n",
    "merged_df = merged_df[merged_df['SodiumContent'] < threshold_map['SodiumContent']]\n",
    "merged_df = merged_df[merged_df['CarbohydrateContent'] < threshold_map['CarbohydrateContent']]\n",
    "merged_df = merged_df[merged_df['FiberContent'] < threshold_map['FiberContent']]\n",
    "merged_df = merged_df[merged_df['SugarContent'] < threshold_map['SugarContent']]\n",
    "merged_df = merged_df[merged_df['ProteinContent'] < threshold_map['ProteinContent']]\n",
    "\n",
    "\n",
    "# -> Drop unimportant columns \n",
    "merged_df = merged_df.drop(columns=['AuthorId', 'RecipeId', 'TotalTime_Requested', 'TotalTime_Recipe', 'RecipeServings', 'RecipeCategory', 'SaturatedFatContent', 'CholesterolContent', 'FiberContent', 'SugarContent', 'LowSugar_Requested', 'HighFiber_Requested', 'Age'])\n",
    "# One-Hot_encoding for categorical columns\n",
    "merged_df = pd.get_dummies(merged_df, columns=['user_diet_category', 'recipe_diet_category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 115349 entries, 0 to 140194\n",
      "Data columns (total 16 columns):\n",
      " #   Column                           Non-Null Count   Dtype  \n",
      "---  ------                           --------------   -----  \n",
      " 0   HighCalories_Requested           115349 non-null  boolean\n",
      " 1   HighProtein_Requested            115349 non-null  boolean\n",
      " 2   LowFat_Requested                 115349 non-null  boolean\n",
      " 3   Calories                         115349 non-null  float64\n",
      " 4   FatContent                       115349 non-null  float64\n",
      " 5   SodiumContent                    115349 non-null  float64\n",
      " 6   CarbohydrateContent              115349 non-null  float64\n",
      " 7   ProteinContent                   115349 non-null  float64\n",
      " 8   Like                             80145 non-null   boolean\n",
      " 9   TestSetId                        35204 non-null   float64\n",
      " 10  user_diet_category_Omnivore      115349 non-null  bool   \n",
      " 11  user_diet_category_Vegan         115349 non-null  bool   \n",
      " 12  user_diet_category_Vegetarian    115349 non-null  bool   \n",
      " 13  recipe_diet_category_Omnivore    115349 non-null  bool   \n",
      " 14  recipe_diet_category_Vegan       115349 non-null  bool   \n",
      " 15  recipe_diet_category_Vegetarian  115349 non-null  bool   \n",
      "dtypes: bool(6), boolean(4), float64(6)\n",
      "memory usage: 7.7 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighCalories_Requested</th>\n",
       "      <th>HighProtein_Requested</th>\n",
       "      <th>LowFat_Requested</th>\n",
       "      <th>Calories</th>\n",
       "      <th>FatContent</th>\n",
       "      <th>SodiumContent</th>\n",
       "      <th>CarbohydrateContent</th>\n",
       "      <th>ProteinContent</th>\n",
       "      <th>Like</th>\n",
       "      <th>TestSetId</th>\n",
       "      <th>user_diet_category_Omnivore</th>\n",
       "      <th>user_diet_category_Vegan</th>\n",
       "      <th>user_diet_category_Vegetarian</th>\n",
       "      <th>recipe_diet_category_Omnivore</th>\n",
       "      <th>recipe_diet_category_Vegan</th>\n",
       "      <th>recipe_diet_category_Vegetarian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HighCalories_Requested  HighProtein_Requested  LowFat_Requested  Calories  \\\n",
       "0                   False                  False             False     241.3   \n",
       "1                    True                   True              True     241.3   \n",
       "2                    True                  False             False     241.3   \n",
       "3                    True                   True              True     241.3   \n",
       "4                   False                   True              True     241.3   \n",
       "\n",
       "   FatContent  SodiumContent  CarbohydrateContent  ProteinContent   Like  \\\n",
       "0        10.1           13.1                 31.8             6.7  False   \n",
       "1        10.1           13.1                 31.8             6.7  False   \n",
       "2        10.1           13.1                 31.8             6.7  False   \n",
       "3        10.1           13.1                 31.8             6.7  False   \n",
       "4        10.1           13.1                 31.8             6.7  False   \n",
       "\n",
       "   TestSetId  user_diet_category_Omnivore  user_diet_category_Vegan  \\\n",
       "0        NaN                        False                     False   \n",
       "1        NaN                        False                     False   \n",
       "2        NaN                        False                     False   \n",
       "3        NaN                        False                      True   \n",
       "4        NaN                        False                      True   \n",
       "\n",
       "   user_diet_category_Vegetarian  recipe_diet_category_Omnivore  \\\n",
       "0                           True                          False   \n",
       "1                           True                          False   \n",
       "2                           True                          False   \n",
       "3                          False                          False   \n",
       "4                          False                          False   \n",
       "\n",
       "   recipe_diet_category_Vegan  recipe_diet_category_Vegetarian  \n",
       "0                       False                             True  \n",
       "1                       False                             True  \n",
       "2                       False                             True  \n",
       "3                       False                             True  \n",
       "4                       False                             True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.info()\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Spliting : Test - Train - Val \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "- randomly split with shuffle=True  (Note: remember the random_state number to be able to reproduce the split) \n",
    "- k-cross validation? \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove  HighCalories_Requested             10157\n",
      "HighProtein_Requested              10157\n",
      "LowFat_Requested                   10157\n",
      "Calories                           10157\n",
      "FatContent                         10157\n",
      "SodiumContent                      10157\n",
      "CarbohydrateContent                10157\n",
      "ProteinContent                     10157\n",
      "Like                               10157\n",
      "user_diet_category_Omnivore        10157\n",
      "user_diet_category_Vegan           10157\n",
      "user_diet_category_Vegetarian      10157\n",
      "recipe_diet_category_Omnivore      10157\n",
      "recipe_diet_category_Vegan         10157\n",
      "recipe_diet_category_Vegetarian    10157\n",
      "dtype: int64 duplicate rows\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 62989 entries, 54607 to 134615\n",
      "Data columns (total 14 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   HighCalories_Requested           62989 non-null  boolean\n",
      " 1   HighProtein_Requested            62989 non-null  boolean\n",
      " 2   LowFat_Requested                 62989 non-null  boolean\n",
      " 3   Calories                         62989 non-null  float64\n",
      " 4   FatContent                       62989 non-null  float64\n",
      " 5   SodiumContent                    62989 non-null  float64\n",
      " 6   CarbohydrateContent              62989 non-null  float64\n",
      " 7   ProteinContent                   62989 non-null  float64\n",
      " 8   user_diet_category_Omnivore      62989 non-null  bool   \n",
      " 9   user_diet_category_Vegan         62989 non-null  bool   \n",
      " 10  user_diet_category_Vegetarian    62989 non-null  bool   \n",
      " 11  recipe_diet_category_Omnivore    62989 non-null  bool   \n",
      " 12  recipe_diet_category_Vegan       62989 non-null  bool   \n",
      " 13  recipe_diet_category_Vegetarian  62989 non-null  bool   \n",
      "dtypes: bool(6), boolean(3), float64(5)\n",
      "memory usage: 3.6 MB\n",
      "Train:\n",
      "Like\n",
      "0    55332\n",
      "1     7657\n",
      "Name: count, dtype: int64\n",
      "Val:\n",
      "Like\n",
      "0    6172\n",
      "1     827\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TrainVal vs. Test split\n",
    "test_dataframe = merged_df[merged_df['TestSetId'].notna()]\n",
    "#test_dataframe.head(100)\n",
    "\n",
    "# Train vs. Val split\n",
    "train_val_dataframe = merged_df[merged_df['TestSetId'].isna()]\n",
    "\n",
    "# Prepare train val for training \n",
    "train_val_dataframe = merged_df[merged_df['Like'].notna()]\n",
    "train_val_dataframe = train_val_dataframe.drop('TestSetId', axis=1)\n",
    "# find duplicated rows in the dataframe and drop\n",
    "print(\"remove \", train_val_dataframe[train_val_dataframe.duplicated()].count(), \"duplicate rows\")\n",
    "train_val_dataframe = train_val_dataframe.drop_duplicates()\n",
    "\n",
    "# put Target (Like column) at the end \n",
    "like_column = train_val_dataframe.pop('Like')\n",
    "train_val_dataframe['Like'] = like_column\n",
    "train_val_dataframe['Like'] = train_val_dataframe['Like'].astype(int)\n",
    "#train_val_dataframe.head(100)\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "  train_test_split(train_val_dataframe.iloc[:, :-1], train_val_dataframe.iloc[:, -1:],\n",
    "                   test_size=0.1, \n",
    "                   shuffle=True,\n",
    "                   random_state=3)\n",
    "\n",
    "X_train.info()\n",
    "#X_val.head()\n",
    "#y_train.info()\n",
    "#y_val.head()\n",
    "\n",
    "# count like of train\n",
    "print('Train:')\n",
    "print(y_train['Like'].value_counts())\n",
    "print('Val:')\n",
    "print(y_val['Like'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Ãnderung: \n",
    "Bei meta_parameter_grid wurde hinzugefÃ¼gt:\n",
    "- parameter_grid_gaussianNB\n",
    "- parameter_grid_linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 86\u001b[0m\n\u001b[1;32m     77\u001b[0m search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline,\n\u001b[1;32m     78\u001b[0m                       meta_parameter_grid, \n\u001b[1;32m     79\u001b[0m                       scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m                       error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# here, the actual training and grid search happens\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest parameter:\u001b[39m\u001b[38;5;124m\"\u001b[39m, search\u001b[38;5;241m.\u001b[39mbest_params_ ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(CV score=\u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/miniforge3/envs/baml/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baml/lib/python3.10/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/baml/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baml/lib/python3.10/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/baml/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baml/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baml/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/baml/lib/python3.10/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB , MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_logistic_regression = LogisticRegression(max_iter=30)\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_gradient_boosting = GradientBoostingClassifier()\n",
    "model_gauusianNB = GaussianNB()\n",
    "model_linearSVC = LinearSVC()\n",
    "\n",
    "# data scaling\n",
    "transform_scaler = StandardScaler()\n",
    "\n",
    "# dimensionality reduction\n",
    "transform_pca = PCA()\n",
    "\n",
    "# train the models\n",
    "pipeline = Pipeline(steps=[(\"scaler\", transform_scaler), \n",
    "                           (\"pca\", transform_pca),\n",
    "                           (\"model\", None)])\n",
    "\n",
    "parameter_grid_preprocessing = {\n",
    "  \"pca__n_components\" : [13, 14],\n",
    "}\n",
    "\n",
    "parameter_grid_gaussianNB = {\n",
    "  \"model\" : [model_gauusianNB],\n",
    "  \"model__var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "\n",
    "# parameter_grid_linearSVC = {\n",
    "#  \"model\" : [model_linearSVC],\n",
    "#  \"model__C\": [0.1, 1, 10],  # Regularization parameter\n",
    "#  \"model__kernels\": ['linear', 'rbf'],  # Kernel type\n",
    "#  \"model__gamma\": [0.1, 1, 10]  # Kernel coefficient for 'rbf'\n",
    "# }\n",
    "parameter_grid_linearSVC = {\n",
    "    \"model\": [model_linearSVC],\n",
    "    \"model__C\": [0.1, 1, 10]  # Regularization parameter\n",
    "}\n",
    "\n",
    "\n",
    "parameter_grid_logistic_regression = {\n",
    "  \"model\" : [model_logistic_regression],\n",
    "  \"model__C\" : [0.1, 1, 10],  # inverse regularization strength\n",
    "}\n",
    "\n",
    "parameter_grid_gradient_boosting = {\n",
    "  \"model\" : [model_gradient_boosting],\n",
    "  \"model__n_estimators\" : [10, 20, 30]\n",
    "}\n",
    "\n",
    "parameter_grid_random_forest = {\n",
    "  \"model\" : [model_random_forest],\n",
    "  \"model__n_estimators\" : [10, 20, 50],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [12, 13],\n",
    "}\n",
    "\n",
    "meta_parameter_grid = [parameter_grid_logistic_regression,\n",
    "                       parameter_grid_random_forest,\n",
    "                       parameter_grid_gradient_boosting,\n",
    "                       parameter_grid_gaussianNB,\n",
    "                       parameter_grid_linearSVC\n",
    "]\n",
    "\n",
    "meta_parameter_grid = [{**parameter_grid_preprocessing, **model_grid}\n",
    "                       for model_grid in meta_parameter_grid]\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      meta_parameter_grid, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=2, \n",
    "                      cv=5,  # number of folds for cross-validation \n",
    "                      error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# here, the actual training and grid search happens\n",
    "search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(\"best parameter:\", search.best_params_ ,\"(CV score=%0.3f)\" % search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test set: 0.5739818241966528\n",
      "true     0     1\n",
      "pred            \n",
      "0     8278  1074\n",
      "1      170   217\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of model on test set\n",
    "print(\"Score on test set:\", search.score(X_val, y_val.values.ravel()))\n",
    "\n",
    "# prediction and show contingency table\n",
    "ct = pd.crosstab(search.best_estimator_.predict(X_val), y_val.values.ravel(),\n",
    "                 rownames=[\"pred\"], colnames=[\"true\"])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': LogisticRegression(max_iter=30), 'model__C': 0.1, 'pca__n_components': 13} 0.501821998328961\n",
      "{'model': LogisticRegression(max_iter=30), 'model__C': 0.1, 'pca__n_components': 14} 0.501821998328961\n",
      "{'model': LogisticRegression(max_iter=30), 'model__C': 1, 'pca__n_components': 13} 0.50195140418955\n",
      "{'model': LogisticRegression(max_iter=30), 'model__C': 1, 'pca__n_components': 14} 0.50195140418955\n",
      "{'model': LogisticRegression(max_iter=30), 'model__C': 10, 'pca__n_components': 13} 0.50195140418955\n",
      "{'model': LogisticRegression(max_iter=30), 'model__C': 10, 'pca__n_components': 14} 0.50195140418955\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 12, 'model__n_estimators': 10, 'pca__n_components': 13} 0.5609268322021761\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 12, 'model__n_estimators': 10, 'pca__n_components': 14} 0.5555012650925621\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 12, 'model__n_estimators': 20, 'pca__n_components': 13} 0.5564193652080455\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 12, 'model__n_estimators': 20, 'pca__n_components': 14} 0.553398545117506\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 12, 'model__n_estimators': 50, 'pca__n_components': 13} 0.5548366405674947\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 12, 'model__n_estimators': 50, 'pca__n_components': 14} 0.5527322670977803\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 13, 'model__n_estimators': 10, 'pca__n_components': 13} 0.5672851208991737\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 13, 'model__n_estimators': 10, 'pca__n_components': 14} 0.5645024903994045\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 13, 'model__n_estimators': 20, 'pca__n_components': 13} 0.5637264555121944\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 13, 'model__n_estimators': 20, 'pca__n_components': 14} 0.559471420096418\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 13, 'model__n_estimators': 50, 'pca__n_components': 13} 0.5599256894652791\n",
      "{'model': RandomForestClassifier(max_depth=13, n_estimators=10), 'model__max_depth': 13, 'model__n_estimators': 50, 'pca__n_components': 14} 0.558967007650871\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 10, 'pca__n_components': 13} 0.5\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 10, 'pca__n_components': 14} 0.5\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 20, 'pca__n_components': 13} 0.5026384390110443\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 20, 'pca__n_components': 14} 0.5026384390110443\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 30, 'pca__n_components': 13} 0.5087863742843789\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 30, 'pca__n_components': 14} 0.5087863742843789\n",
      "{'model': GaussianNB(), 'model__var_smoothing': 1e-09, 'pca__n_components': 13} 0.5186086041891268\n",
      "{'model': GaussianNB(), 'model__var_smoothing': 1e-09, 'pca__n_components': 14} 0.5186086041891268\n",
      "{'model': GaussianNB(), 'model__var_smoothing': 1e-08, 'pca__n_components': 13} 0.5186086041891268\n",
      "{'model': GaussianNB(), 'model__var_smoothing': 1e-08, 'pca__n_components': 14} 0.5186086041891268\n",
      "{'model': GaussianNB(), 'model__var_smoothing': 1e-07, 'pca__n_components': 13} 0.5186151714709288\n",
      "{'model': GaussianNB(), 'model__var_smoothing': 1e-07, 'pca__n_components': 14} 0.5186151714709288\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# (optional, if you're curious) for a detailed look on the performance of the different models\n",
    "def get_search_score_overview():\n",
    "  for c,s in zip(search.cv_results_[\"params\"],search.cv_results_[\"mean_test_score\"]):\n",
    "      print(c, s)\n",
    "\n",
    "print(get_search_score_overview())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighCalories_Requested</th>\n",
       "      <th>HighProtein_Requested</th>\n",
       "      <th>LowFat_Requested</th>\n",
       "      <th>Calories</th>\n",
       "      <th>FatContent</th>\n",
       "      <th>SodiumContent</th>\n",
       "      <th>CarbohydrateContent</th>\n",
       "      <th>ProteinContent</th>\n",
       "      <th>user_diet_category_Omnivore</th>\n",
       "      <th>user_diet_category_Vegan</th>\n",
       "      <th>user_diet_category_Vegetarian</th>\n",
       "      <th>recipe_diet_category_Omnivore</th>\n",
       "      <th>recipe_diet_category_Vegan</th>\n",
       "      <th>recipe_diet_category_Vegetarian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    HighCalories_Requested  HighProtein_Requested  LowFat_Requested  Calories   \n",
       "5                    False                   True             False     241.3  \\\n",
       "7                    False                  False             False     241.3   \n",
       "8                    False                  False             False     241.3   \n",
       "14                   False                   True             False     241.3   \n",
       "15                   False                   True             False     241.3   \n",
       "\n",
       "    FatContent  SodiumContent  CarbohydrateContent  ProteinContent   \n",
       "5         10.1           13.1                 31.8             6.7  \\\n",
       "7         10.1           13.1                 31.8             6.7   \n",
       "8         10.1           13.1                 31.8             6.7   \n",
       "14        10.1           13.1                 31.8             6.7   \n",
       "15        10.1           13.1                 31.8             6.7   \n",
       "\n",
       "    user_diet_category_Omnivore  user_diet_category_Vegan   \n",
       "5                         False                      True  \\\n",
       "7                         False                     False   \n",
       "8                         False                     False   \n",
       "14                        False                      True   \n",
       "15                         True                     False   \n",
       "\n",
       "    user_diet_category_Vegetarian  recipe_diet_category_Omnivore   \n",
       "5                           False                          False  \\\n",
       "7                            True                          False   \n",
       "8                            True                          False   \n",
       "14                          False                          False   \n",
       "15                          False                          False   \n",
       "\n",
       "    recipe_diet_category_Vegan  recipe_diet_category_Vegetarian  \n",
       "5                        False                             True  \n",
       "7                        False                             True  \n",
       "8                        False                             True  \n",
       "14                       False                             True  \n",
       "15                       False                             True  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare test data for prediction\n",
    "test_set_id = test_dataframe.pop('TestSetId')\n",
    "test_dataframe = test_dataframe.drop('Like', axis=1)\n",
    "test_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "model = search.best_estimator_ \n",
    "test_dataframe[\"Like\"] = model.predict(test_dataframe)\n",
    "\n",
    "#TODO: \n",
    "\n",
    "# prediction := List if Like \n",
    "# test_set_id := List of test ID\n",
    "\n",
    "# write to CSV file in the same order  (den Code unten anpassenm)\n",
    "# 1.ID  1.Like \n",
    "# 2.ID  2.Like\n",
    "\n",
    "output = pd.DataFrame(test_dataframe[\"Like\"])\n",
    "output[\"id\"] = test_set_id.astype(\"int\")\n",
    "\n",
    "output = output.rename(columns={'Like': 'prediction'})\n",
    "output = output.reindex(columns=[\"id\", \"prediction\"])\n",
    "\n",
    "output.to_csv('recipe_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume that our id column is the index of the dataframe\n",
    "\n",
    "# print(test_dataframe)\n",
    "#output = pd.DataFrame(test_dataframe[\"Like\"])\n",
    "# output = output.reset_index(drop=True)\n",
    "#output[\"id\"] = output.index + 1\n",
    "#output = output.rename(columns={'Like': 'prediction'})\n",
    "#output = output.reindex(columns=[\"id\", \"prediction\"])\n",
    "# output length\n",
    "#print(len(output))\n",
    "#output.to_csv('recipe_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
